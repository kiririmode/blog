---
Title: RAG：LLMが持っていない独自の知識を使うには
Category:
- ai
Date: 2023-11-18T17:41:08+09:00
URL: https://kiririmode.hatenablog.jp/entry/20231118/1700296868
EditURL: https://blog.hatena.ne.jp/kiririmode/kiririmode.hatenablog.jp/atom/entry/6801883189059798115
---

最近はどこもかしこもGenerative AIの情報で溢れるようになってきています。その中でもよく聞くのが、LLMが未学習である情報（例えば、企業等の組織内のデータ）を学習させ、それを元にした利用がしたいという話です。

[:contents]

# LLMが未学習の知識を利用したい

私の知る限り、大きく分けて2つの方法があります。

## RAG　（Retrieval-Augumented Generation)

RAGは、LLMに学習させるというよりは、質問に回答できるだけの知識をLLMに一時的に与える仕組みです。

例えば2023/11現在の日本の総理大臣の名前を学習していないLLMがあるとします。このLLMに対し「2023/11現在の日本の総理大臣の名前」を聞いたところで回答は得られません。しかし、質問に必要な知識を、以下のようにプロンプトの中で示したらどうでしょうか。

```
2023/11現在の日本の総理大臣の名前を教えてください。

ただし、以下のコンテキストを前提とします。

- 2020〜2021 菅義偉
- 2021〜在任中 岸田文雄
```

LLMは容易に「2023年11月現在、日本の総理大臣は岸田文雄です」と回答してくれました。これが検索拡張生成とも呼ばれる「RAG」の原理です。ポイントは、ユーザの質問から「LLMに渡すべきコンテキストを抽出し整形する」仕組みが必要であることで、概形としては下図のような形になります。

f:id:kiririmode:20231119182452p:image

Knowledge Baseと記載した箇所は、ユースケースによってはWikipediaにもなるでしょうし、社内文書の内容を格納したデータベースにもなるでしょう。

- [https://tech.connehito.com/entry/2023/11/14/221416:title]
- [https://zenn.dev/sompojapan_dx/articles/eb755a18e893ce:title]
- [https://xtech.nikkei.com/atcl/nxt/column/18/00001/08332/:title]

## Fine-Tuning

もう1つの方法はFine-Tuningです。

こちらはLLMに対して、特定のデータセットを追加学習させるという方法です。
具体的なイメージが掴みにくいところですが、[https://platform.openai.com/docs/guides/fine-tuning:title]を見るとわかりやすいです。現時点で記載のある学習用データセットとなるJSONファイル例が以下です。

```json
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "Paris, as if everyone doesn't know that already."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "Oh, just some guy named William Shakespeare. Ever heard of him?"}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "Around 384,400 kilometers. Give or take a few, like that really matters."}]}
```

このようなデータセットを用意して、[Fine-Tuning用のAPI](https://platform.openai.com/docs/api-reference/fine-tuning/create)を呼び出せば、Fine-Tuningされたモデルが構築できます。

どのくらいの学習用データが必要なのかはケースバイケースですが、OpenAIでは50個くらいから始めろと示唆されています。

> We recommend starting with 50 well-crafted demonstrations and seeing if the model shows signs of improvement after fine-tuning.
>
> <cite>[https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset:title]</cite>

# どっちを選べば良いのだろうか

回答品質を向上させる方法は他にもある上、RAGとFine-Tuningも排他的なオプションではありません。
このため、必ずしも二元論ではないのですが、私の知る限り、次のような順で試すことが良いようです。

1. RAG
2. Fine-Tuning

ちなみに、回答品質向上のためには、質問に関して適切なプロンプトが入力されることを前提としているため、どちらについてもプロンプトエンジニアリングは必要です。

## なぜ先にRAGなのか

## Fine-Tuningに時間がかかる

Fine-Tuningを行い回答品質を向上させるためには、シンプルに時間がかかるからです。

時間がかかるという意味で言えばRAGも同様なのですが、Fine-Tuningを行う場合、ユースケースごとに質問と回答のペアを慎重かつ根気強く提供する必要があります。ナレッジベースが増えれば増えるほど、ユースケースは拡大し、Fine-Tuningするときの学習データも増えるでしょう。
一方で学習データが不足すると、下記に示すハルシネーションや、オーバーフィッティングの可能性も高くなります。

## ハルシネーション

また、質問に関する知識の学習をしていないLLMは、ハルシネーションを起こしやすいです。

[https://aiboom.net/archives/58767:title]ではハルシネーションが何に起因するのかをまとめており、そのうちの1つが知識の欠損です。

> LLMは広範な事実知識を保有していますが、限界があります。特定の専門分野の知識が欠けていたり、最新の事実知識が不足していることがあり、それがハルシネーションを引き起こす可能性があります。

RAGでは、知識をKnowledge Baseから引き出しLLMに与えた上で回答を抽出するため、このハルシネーションが比較的起こりにくいようです。

## デバッグの容易さ

RAGは、回答品質が良くない場合のデバッグも容易です。なぜなら、どのようなコンテキスト情報をLLMに提供したのか、LLMに与えたプロンプトから覗き見ることが可能であるからです。

# 参考文献

- [https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7:title]
- [https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/:title]
